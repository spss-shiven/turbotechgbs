# -*- coding: utf-8 -*-
"""Shivendra Sisodiya | Assessment for Turbotech GBS

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eibHV1jgg6c4POk-IWsZ_Zl8g3dttnD7

#Data Prepration and Exploration

I have gathered the dataset of some fake and real news data. The link for the dataset is - https://github.com/diptamath/covid_fake_news/blob/main/data/english_test_with_labels.csv
"""

# Step 1: Install necessary libraries
!pip install pandas nltk spacy

# Download spaCy language model
!python -m spacy download en_core_web_sm

# Step 2: Import necessary libraries
import pandas as pd
import nltk
import re
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import spacy

# Step 3: Load spaCy model
nlp = spacy.load("en_core_web_sm")

# Step 4: Upload your CSV file
from google.colab import files
uploaded = files.upload()

# After uploading, specify your file name
file_name = list(uploaded.keys())[0]  # Get the first uploaded file name

# Step 5: Load CSV file into a DataFrame
df = pd.read_csv(file_name)

import nltk
nltk.download('stopwords')
nltk.download('punkt_tab')

# Step 6: Define text preprocessing function
# Preprocessing like lowercasing, removing non-alphabetic characters, tokenizing
stop_words = set(stopwords.words('english'))

def preprocess_text(text):
    # Convert text to lowercase
    text = text.lower()
    # Remove non-alphabetic characters (including numbers and special characters)
    text = re.sub(r'[^a-zA-Z\s]', '', text)
    # Tokenize the text
    tokens = word_tokenize(text)
    # Remove stopwords
    tokens = [word for word in tokens if word not in stop_words]
    # Lemmatize using spaCy
    doc = nlp(' '.join(tokens))
    lemmatized = [token.lemma_ for token in doc]
    return ' '.join(lemmatized)

# Step 7: Apply preprocessing to the text column
text_column = 'tweet'
df['processed_text'] = df[text_column].astype(str).apply(preprocess_text)

# Step 8: Preview the processed data
df[['processed_text']].head()

# Step 9: Save the processed dataframe to a new CSV
df.to_csv('processed_output.csv', index=False)

# Step 10: Download the processed CSV file
files.download('processed_output.csv')

"""#Exploratory Analysis of the text."""

import pandas as pd
import nltk
import matplotlib.pyplot as plt
from collections import Counter
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize, sent_tokenize
from wordcloud import WordCloud

# Load data
from google.colab import files
uploaded = files.upload()

# Load the CSV into a DataFrame
file_name = list(uploaded.keys())[0]
df = pd.read_csv(file_name)

# Choose your text column
text_column = 'tweet'

# Step 1: Basic text statistics (word count, sentence count)
def get_basic_text_stats(text):
    num_words = len(word_tokenize(text))  # Tokenize words
    num_sentences = len(sent_tokenize(text))  # Tokenize sentences
    return num_words, num_sentences

df['num_words'], df['num_sentences'] = zip(*df[text_column].astype(str).apply(get_basic_text_stats))

# Display basic stats
print(f"Total number of rows: {len(df)}")
print(f"Average number of words per document: {df['num_words'].mean()}")
print(f"Average number of sentences per document: {df['num_sentences'].mean()}")

# Step 2: Visualize word count distribution (histogram)
plt.figure(figsize=(10, 6))
plt.hist(df['num_words'], bins=30, color='skyblue', edgecolor='black')
plt.title('Word Count Distribution')
plt.xlabel('Number of Words')
plt.ylabel('Frequency')
plt.show()

# Step 3: Find the most frequent words in the entire dataset (excluding stopwords)
stop_words = set(stopwords.words('english'))

# Tokenize all words in the dataset and filter stopwords
all_words = []
for text in df[text_column]:
    words = word_tokenize(str(text).lower())
    filtered_words = [word for word in words if word.isalpha() and word not in stop_words]
    all_words.extend(filtered_words)

# Get the top 20 most common words
word_counts = Counter(all_words)
top_20_words = word_counts.most_common(20)
print("Top 20 Most Frequent Words:", top_20_words)

# Step 4: Create a Word Cloud for Visualization
wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_counts)

plt.figure(figsize=(10, 6))
plt.imshow(wordcloud, interpolation='bilinear')
plt.title('Word Cloud of Most Frequent Words')
plt.axis('off')
plt.show()

# Step 5: Plot text length distribution (characters)
df['text_length'] = df[text_column].astype(str).apply(len)

plt.figure(figsize=(10, 6))
plt.hist(df['text_length'], bins=30, color='salmon', edgecolor='black')
plt.title('Text Length Distribution (in Characters)')
plt.xlabel('Text Length (Characters)')
plt.ylabel('Frequency')
plt.show()

# Step 6: Count the number of stopwords in the dataset
stopword_count = df[text_column].apply(lambda text: len([word for word in word_tokenize(str(text).lower()) if word in stop_words]))
print(f"Average number of stopwords per document: {stopword_count.mean()}")

# Step 7: Calculate average sentence length (in words)
df['avg_sentence_length'] = df['num_words'] / df['num_sentences']
print(f"Average sentence length (in words): {df['avg_sentence_length'].mean()}")

# Step 8: Visualize the average sentence length
plt.figure(figsize=(10, 6))
plt.hist(df['avg_sentence_length'], bins=30, color='green', edgecolor='black')
plt.title('Average Sentence Length Distribution')
plt.xlabel('Average Sentence Length (Words)')
plt.ylabel('Frequency')
plt.show()

"""#Entity Extraction

"""

# Install necessary libraries
!pip install pandas nltk spacy transformers wordcloud sklearn
!python -m spacy download en_core_web_sm
!pip install sumy

import re

def extract_dates(text):
    # Extract dates in formats like "dd-mm-yyyy" or "dd/mm/yyyy"
    date_pattern = r'\b(\d{2}[/-]\d{2}[/-]\d{4})\b'
    dates = re.findall(date_pattern, text)
    return dates

def extract_emails(text):
    # Extract emails
    email_pattern = r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,7}\b'
    emails = re.findall(email_pattern, text)
    return emails

# Apply the functions to your text column
df['dates'] = df['tweet'].apply(extract_dates)
df['emails'] = df['tweet'].apply(extract_emails)

import spacy

# Load spaCy model
nlp = spacy.load("en_core_web_sm")

# Function to extract Named Entities (NER) and POS tags
def extract_named_entities(text):
    doc = nlp(text)
    entities = [(entity.text, entity.label_) for entity in doc.ents]
    return entities

def extract_pos_tags(text):
    doc = nlp(text)
    pos_tags = [(token.text, token.pos_) for token in doc]
    return pos_tags

# Apply the functions to your text column
df['named_entities'] = df['tweet'].apply(extract_named_entities)
df['pos_tags'] = df['tweet'].apply(extract_pos_tags)

from transformers import pipeline

# Initialize Hugging Face BART model for summarization
summarizer = pipeline("summarization", model="facebook/bart-large-cnn")

# Abstractive Summarization Function
def abstractive_summary(text, min_length=30, max_length=150):
    summary = summarizer(text, min_length=min_length, max_length=max_length)
    return summary[0]['summary_text']

# Apply summarization to your text column
df['abstractive_summary'] = df['tweet'].apply(abstractive_summary)

